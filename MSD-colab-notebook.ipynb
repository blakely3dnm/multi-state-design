#!!!!!!!!WORK IN PROGRESS!!!!!! - - This notebook has been tested on a local colab runtime

#USE CASE - Engineer ligand-induced dimerization starting from a monomeric, conformationally responsive protein-ligand system.
# Human glucokinase (hGK) is used as an example.hGK is monomeric and increasingly samples a closed conformation upon glucose binding.
# This notebook generates and validates candidate sequences on the condition that each sequence is compatible with both a monomeric/open
# conformation and a closed/dimeric conformation.

# How - Automatically loads specified PDBs of open and closed hGK. Using RfDiffusion, the input PDBs are automatically inpainted 
#     and brought to the same length. A third input PDB is loaded which contains a homologous protein in dimeric state, which is used
#     as a template for generating a model of hGK in a closed, dimeric complex. ProteinMPNN is run with --homooligomer to fix the identity of
      residue pairs across open annd closed states. Finally sequences are validated using AF2 and combined MSD metrics are calculated.

# - This is essentially cutting, taping, and pasting from colabdesign examples, and relying on their underlying code with minimal adaptations,
# beyond establishing the workflow. https://github.com/sokrypton/ColabDesign

#@title Setup **RFdiffusion** (~3min)
%%time
import os, time, signal, sys, subprocess
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import display
import ipywidgets as widgets

# --- Install RFdiffusion and dependencies ---
if not os.path.isdir("params"):
    os.system("apt-get install aria2 -y")
    os.system("mkdir params")
    os.system("(\
aria2c -q -x 16 https://files.ipd.uw.edu/krypton/schedules.zip; \
aria2c -q -x 16 http://files.ipd.uw.edu/pub/RFdiffusion/6f5902ac237024bdd0c176cb93063dc4/Base_ckpt.pt; \
aria2c -q -x 16 http://files.ipd.uw.edu/pub/RFdiffusion/e29311f6f1bf1af907f9ef9f44b8328b/Complex_base_ckpt.pt; \
aria2c -q -x 16 https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar; \
tar -xf alphafold_params_2022-12-06.tar -C params; \
touch params/done.txt) &")

if not os.path.isdir("RFdiffusion"):
    print("installing RFdiffusion...")
    os.system("git clone https://github.com/sokrypton/RFdiffusion.git")
    os.system("pip install jedi omegaconf hydra-core icecream pyrsistent pynvml decorator")
    os.system("pip install git+https://github.com/NVIDIA/dllogger#egg=dllogger")
    os.system("pip install --no-dependencies dgl==2.0.0 -f https://data.dgl.ai/wheels/cu121/repo.html")
    os.system("pip install --no-dependencies e3nn==0.5.5 opt_einsum_fx")
    os.system("cd RFdiffusion/env/SE3Transformer && pip install .")
    os.system("wget -qnc https://files.ipd.uw.edu/krypton/ananas")
    os.system("chmod +x ananas")

if not os.path.isdir("colabdesign"):
    print("installing ColabDesign...")
    os.system("pip install git+https://github.com/sokrypton/ColabDesign.git@v1.1.1")
    os.system("ln -s /usr/local/lib/python3.*/dist-packages/colabdesign colabdesign")

if not os.path.isdir("RFdiffusion/models"):
    print("downloading RFdiffusion params...")
    os.system("mkdir -p RFdiffusion/models")
    models = ["Base_ckpt.pt", "Complex_base_ckpt.pt"]
    for m in models:
        while os.path.isfile(f"{m}.aria2"):
            time.sleep(5)
    os.system(f"mv {' '.join(models)} RFdiffusion/models")
    os.system("unzip -o schedules.zip && rm schedules.zip")

if 'RFdiffusion' not in sys.path:
    os.environ["DGLBACKEND"] = "pytorch"
    sys.path.append('RFdiffusion')

# --- Imports ---
from inference.utils import parse_pdb
from colabdesign.rf.utils import get_ca, sym_it, fix_contigs, fix_partial_contigs, fix_pdb
from colabdesign.shared.protein import pdb_to_string

seq_csv = "output/mpnn_results.csv"


# --- Utility functions ---
def run(cmd, steps, num_designs=1, visual="none"):
    progress = widgets.FloatProgress(min=0, max=steps, description='running', bar_style='info')
    display(progress)
    print(f"Running command:\n{cmd}")
    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    try:
        for step in range(steps):
            time.sleep(1)
            progress.value = step + 1
            if proc.poll() is not None:
                break
        stdout, stderr = proc.communicate(timeout=600)
        print("--- RFdiffusion STDOUT ---\n", stdout)
        print("--- RFdiffusion STDERR ---\n", stderr)
        if proc.returncode != 0:
            raise RuntimeError(f"RFdiffusion failed with code {proc.returncode}")
    except subprocess.TimeoutExpired:
        proc.kill()
        raise

def run_diffusion(contigs, path, pdb=None, iterations=50,
                  symmetry="none", order=1, hotspot=None,
                  chains=None, add_potential=False,
                  num_designs=1, visual="none"):
    full_path = f"outputs/{path}"
    os.makedirs(full_path, exist_ok=True)
    opts = [f"inference.output_prefix={full_path}",
            f"inference.num_designs={num_designs}"]

    if chains == "":
        chains = None

    contigs_str = contigs if isinstance(contigs, str) else " ".join(contigs)
    def is_partial_contig(c): return "/" in c and c.split("/")[0].isdigit() and c.split("/")[1].isalpha()
    if all(is_partial_contig(c) for c in contigs_str.split()): mode = "partial"
    elif any("-" in c for c in contigs_str.split()): mode = "fixed"
    else: mode = "free"

    if mode in ["partial", "fixed"]:
        pdb_str = pdb_to_string(pdb, chains=chains)
        pdb_filename = f"{full_path}/input.pdb"
        with open(pdb_filename, "w") as f: f.write(pdb_str)
        parsed_pdb = parse_pdb(pdb_filename)
        opts.append(f"inference.input_pdb={pdb_filename}")

    if mode == "partial":
        iterations = int(80 * (iterations / 200))
        opts.append(f"diffuser.partial_T={iterations}")
        contigs_fixed = contigs_str.split()
    else:
        opts.append(f"diffuser.T={iterations}")
        contigs_fixed = fix_contigs(contigs_str.split(), parsed_pdb)

    opts.append(f"'contigmap.contigs=[{'/'.join(contigs_fixed)}]'")
    opts += ["inference.dump_pdb=True", "inference.dump_pdb_path='/dev/shm'"]
    print("mode:", mode)
    print("contigs:", contigs_fixed)
    cmd = f"./RFdiffusion/run_inference.py {' '.join(opts)}"
    run(cmd, iterations, num_designs, visual=visual)
    return contigs_fixed, 1

# --- Inpainting Helpers ---
def download_pdb(pdb_id):
    pdb_id = pdb_id.lower()
    pdb_gz = f"{pdb_id}.pdb1.gz"
    pdb_file = f"{pdb_id}.pdb1"
    if not os.path.exists(pdb_file):
        os.system(f"wget -qnc https://files.rcsb.org/download/{pdb_gz}")
        os.system(f"gunzip -f {pdb_gz}")
    return pdb_file

def get_residues(pdb_path, chain):
    parsed = parse_pdb(pdb_path)
    return sorted(resnum for ch, resnum in parsed["pdb_idx"] if ch == chain)

def get_full_range(res_lists):
    union = sorted(set().union(*res_lists))
    return list(range(min(union), max(union)+1))

def build_contigs(res_present, full_range, chain="A"):
    contigs = []
    i = 0
    while i < len(full_range):
        r = full_range[i]
        if r in res_present:
            j = i
            while j < len(full_range) and full_range[j] in res_present: j += 1
            contigs.append(f"{chain}{full_range[i]}-{full_range[j-1]}")
            i = j
        else:
            j = i
            while j < len(full_range) and full_range[j] not in res_present: j += 1
            length = j - i
            contigs.append(f"{length}-{length}")
            i = j
    return "/".join(contigs)

def iterative_diffusion_infill(pdb_path, ref_residues, chain, tag):
        cur_res = get_residues(pdb_path, chain)
        missing = set(ref_residues) - set(cur_res)
        if not missing:
            print(f"[{tag}] Done: all residues filled.")
        else:
          contig_str = build_contigs(cur_res, ref_residues, chain)
          print(f"[{tag}] Contigs: {contig_str}")
          contigs_fixed, _ = run_diffusion(contig_str, f"{tag}_round1", pdb=pdb_path, iterations=50)
          pdb_path = f"outputs/{tag}_round1_0.pdb"
          return pdb_path

# --- Main Execution ---
pdb1_path = download_pdb("3VEY")
pdb2_path = download_pdb("4DCH")
print(f"Downloaded:\n- {pdb1_path}\n- {pdb2_path}")

chain = "A"
res1 = get_residues(pdb1_path, chain)
res2 = get_residues(pdb2_path, chain)
ref_range = get_full_range([res1, res2])

pdb1_filled = iterative_diffusion_infill(pdb1_path, ref_range, chain, tag="pdb1")
pdb2_filled = iterative_diffusion_infill(pdb2_path, ref_range, chain, tag="pdb2")

print("\n✅ Final filled models:")
print(f"  - {pdb1_filled}")
print(f"  - {pdb2_filled}")
parsed1 = parse_pdb(pdb1_filled)
parsed2 = parse_pdb(pdb2_filled)
print(f" filled 1 length: {parsed1['pdb_idx']}")
print(f" filled 2 length: {parsed1['pdb_idx']}")




import numpy as np
from Bio.PDB import PDBParser, Superimposer, PDBIO
from Bio.PDB.StructureBuilder import StructureBuilder

def download_pdb(pdb_id):
    pdb_id = pdb_id.lower()
    pdb_gz = f"{pdb_id}.pdb1.gz"
    pdb_file = f"{pdb_id}.pdb1"
    if not os.path.exists(pdb_file):
        os.system(f"wget -qnc https://files.rcsb.org/download/{pdb_gz}")
        os.system(f"gunzip -f {pdb_gz}")
    return pdb_file

def extract_chain(structure, chain_id):
    for model in structure:
        for chain in model:
            if chain.id == chain_id:
                return chain.copy()
    raise ValueError(f"Chain '{chain_id}' not found in structure.")


def get_ca_atoms(chain):
    """Get all Cα atoms from a chain."""
    return [atom for res in chain for atom in res if atom.get_id() == "CA"]

def load_structure(pdb_path, name="structure"):
    parser = PDBParser(QUIET=True)
    return parser.get_structure(name, pdb_path)


from Bio.PDB import Superimposer, Atom, Residue, Chain
import copy

def align_and_copy_chain(mobile_chain, ref_chain, new_chain_id):
    """Align a copy of mobile_chain to ref_chain and return transformed copy with new chain ID."""

    si = Superimposer()
    ref_atoms = get_ca_atoms(ref_chain)
    mob_atoms = get_ca_atoms(mobile_chain)

    # Ensure equal length (truncate to shortest)
    min_len = min(len(ref_atoms), len(mob_atoms))
    if min_len == 0:
        raise ValueError("No Cα atoms found for alignment.")

    # Set up alignment
    si.set_atoms(ref_atoms[:min_len], mob_atoms[:min_len])

    # Deep copy the chain before applying transform
    new_chain = Chain.Chain(new_chain_id)
    for residue in mobile_chain:
        new_res = Residue.Residue(residue.id, residue.resname, residue.segid)
        for atom in residue:
            coord = atom.coord
            new_coord = np.dot(coord, si.rotran[0]) + si.rotran[1]
            new_atom = Atom.Atom(atom.name, new_coord, atom.bfactor, atom.occupancy,
                                 atom.altloc, atom.fullname, atom.serial_number, atom.element)
            new_res.add(new_atom)
        new_chain.add(new_res)

    return new_chain

def print_chains(structure, label=""):
    print(f"🔎 Chains in {label}: {[chain.id for model in structure for chain in model]}")


def build_aligned_dimer(template_pdb, pdb2_filled_path, output_pdb="pdb2_aligned_dimer.pdb"):
    """Align two copies of pdb2_filled to chains A and D of template (e.g., 4JAX)."""

    print(f" Downloading {template_pdb}")
    template_pdb = download_pdb(template_pdb)
    print(f" saved template as {template_pdb}")
    template = load_structure(template_pdb, "template")
    mono_struct = load_structure(pdb2_filled_path, "monomer")

    print_chains(template, "template")
    print_chains(mono_struct, "monomer")

    chain_A_template = extract_chain(template, "A")
    chain_D_template = extract_chain(template, "D")
    chain_A_mono = extract_chain(mono_struct, "A")
    chain_B_mono = extract_chain(mono_struct, "A")  # second copy

    aligned_A = align_and_copy_chain(chain_A_mono, chain_A_template, "B")
    aligned_B = align_and_copy_chain(chain_B_mono, chain_D_template, "C")

    # Build new structure with both aligned chains
    builder = StructureBuilder()
    builder.init_structure("aligned_dimer")
    builder.init_model(0)
    builder.structure[0].add(aligned_A)
    builder.structure[0].add(aligned_B)

    # Save output
    io = PDBIO()
    io.set_structure(builder.structure)
    io.save(output_pdb)
    print(f"✅ Aligned dimer saved as: {output_pdb}")

# Example usage:
build_aligned_dimer("4JAX", "/content/outputs/pdb2_round1_0.pdb")

import builtins
from inference.utils import parse_pdb
parser = PDBParser(QUIET=True)

structure1 = parser.get_structure("dimer", "/content/pdb2_aligned_dimer.pdb")
count1 = builtins.sum(1 for _ in structure1.get_residues())
print(f"✅ Residue count in aligned dimer: {count1}")

structure2 = parser.get_structure("fixed", "/content/outputs/pdb1_round1_0.pdb")
count2 = builtins.sum(1 for _ in structure2.get_residues())
print(f"✅ Residue count in cleaned version: {count2}")

import numpy as np
from Bio.PDB import PDBParser, PDBIO, Chain, Model, Structure

def get_residues(pdb_path, chain):
    parsed = parse_pdb(pdb_path)
    return sorted(resnum for ch, resnum in parsed["pdb_idx"] if ch == chain)

def translate_structure(structure, translation):
    """Applies translation to all atoms in the structure."""
    for atom in structure.get_atoms():
        atom.coord += translation
    return structure

def rename_chain(chain, new_id):
    """Renames a chain ID."""
    chain.id = new_id
    return chain

def merge_structures_with_renaming(pdb1_path, pdb2_path, output_path, min_separation=15.0):
    parser = PDBParser(QUIET=True)
    pdb1 = parser.get_structure("pdb1", pdb1_path)
    pdb2 = parser.get_structure("pdb2", pdb2_path)

    # Compute bounding boxes for separation
    pdb1_coords = np.array([atom.coord for atom in pdb1.get_atoms()])
    pdb2_coords = np.array([atom.coord for atom in pdb2.get_atoms()])
    pdb1_max_x = np.max(pdb1_coords[:, 0])
    pdb2_min_x = np.min(pdb2_coords[:, 0])
    shift_x = pdb2_min_x - pdb1_max_x + min_separation

    # Translate pdb1 (monomer) away from dimer
    translated_pdb1 = translate_structure(pdb1, np.array([shift_x, 0, 0]))

    # Create merged structure
    merged_structure = Structure.Structure("merged")
    model = Model.Model(0)

    # Add monomer as chain A
    for chain in translated_pdb1[0]:
        model.add(rename_chain(chain.copy(), "A"))

    # Add dimer as chains B and C
    chains = [rename_chain(chain.copy(), cid) for chain, cid in zip(pdb2[0], ["B", "C"])]
    for chain in chains:
        model.add(chain)

    merged_structure.add(model)

    # Save output
    io = PDBIO()
    io.set_structure(merged_structure)
    io.save(output_path)
    print(f"✅ Merged and renamed PDB saved to: {output_path}")


# Example usage:

output_path = "/content/outputs/merged_open_monomer_and_closed_dimer.pdb"
merge_structures_with_renaming(
    pdb1_path="/content/outputs/pdb1_round1_0.pdb",
    pdb2_path="/content/pdb2_aligned_dimer.pdb",
    output_path=output_path
)
chain_lenA = len(get_residues(output_path, "A"))
chain_lenB = len(get_residues(output_path, "B"))
chain_lenC = len(get_residues(output_path, "C"))
print(f"Chain length A: {chain_lenA}")
print(f"Chain length B: {chain_lenB}")
print(f"Chain length C: {chain_lenC}")








#@title Install colabdesign
import os
print("installing ColabDesign...")
os.system("pip -q install git+https://github.com/sokrypton/ColabDesign.git")
os.system("ln -s /usr/local/lib/python3.*/dist-packages/colabdesign colabdesign")

from colabdesign.mpnn import mk_mpnn_model, clear_mem
from colabdesign.shared.protein import pdb_to_string

import jax
import jax.numpy as jnp
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import HTML
import pandas as pd
import tqdm.notebook
TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'

from google.colab import files
from google.colab import data_table
data_table.enable_dataframe_formatter()


def get_pdb(pdb_code=""):
  if pdb_code is None or pdb_code == "":
    upload_dict = files.upload()
    pdb_string = upload_dict[list(upload_dict.keys())[0]]
    with open("tmp.pdb","wb") as out: out.write(pdb_string)
    return "tmp.pdb"
  elif os.path.isfile(pdb_code):
    return pdb_code
  elif len(pdb_code) == 4:
    os.system(f"wget -qnc https://files.rcsb.org/view/{pdb_code}.pdb")
    return f"{pdb_code}.pdb"
  else:
    os.system(f"wget -qnc https://alphafold.ebi.ac.uk/files/AF-{pdb_code}-F1-model_v3.pdb")
    return f"AF-{pdb_code}-F1-model_v3.pdb"

from inference.utils import parse_pdb
from Bio.PDB.NeighborSearch import NeighborSearch



def get_fixed_residue_indices(pdb_path, ligand_resname="GLC", distance_cutoff=5.0):
    parser = PDBParser(QUIET=True)
    structure = parser.get_structure("3VEY", pdb_path)

    # Flatten all atoms for spatial search
    all_atoms = list(structure.get_atoms())

    # Extract glucose atoms
    glucose_atoms = [
        atom for atom in all_atoms
        if atom.get_parent().get_resname() == ligand_resname
    ]
    if not glucose_atoms:
        raise ValueError(f"No ligand residues named '{ligand_resname}' found in {pdb_path}")

    # Build spatial search tree
    ns = NeighborSearch(all_atoms)

    # Find all atoms within distance_cutoff of glucose atoms
    close_atoms = set()
    for glc_atom in glucose_atoms:
        neighbors = ns.search(glc_atom.coord, distance_cutoff, level="A")  # atom-level
        close_atoms.update(neighbors)

    # Extract residue indices of those atoms (chain, resseq)
    close_residues = set()
    for atom in close_atoms:
        res = atom.get_parent()
        chain_id = res.get_parent().id
        res_id = res.id[1]  # tuple (hetfield, resseq, icode) → use [1] for resseq
        close_residues.add((chain_id, res_id))

    # Sort and convert to list
    sorted_fixed = sorted(close_residues, key=lambda x: (x[0], x[1]))
    print(f"✅ Found {len(sorted_fixed)} residues within {distance_cutoff} Å of {ligand_resname}")
    for chain_id, res_id in sorted_fixed:
        print(f"{chain_id} {res_id}")

    return sorted_fixed


# Given glucose-proximal residues like this:
glucose_residues = get_fixed_residue_indices("3vey.pdb1", "GLC", 5.0)

# Convert to format: A151,B151,C151,... etc
chains = ["A", "B", "C"]
fixed_pos_ids = [f"{chain}{resnum}" for _, resnum in glucose_residues for chain in chains]

# Convert list to comma-separated string
# Parse structure to find which residues exist
parsed = parse_pdb(pdb_path)
existing_residues = set((ch, res) for ch, res in parsed["pdb_idx"])

# Filter to only include residues that exist
chain_list = chains if isinstance(chains, list) else chains.split(",")
valid_fix_pos = [
    f"{c}{r}" for (chain, r) in glucose_residues
    for c in chain_list
    if (c, r) in existing_residues
]
fixed_pos_str = ",".join(valid_fix_pos)


print("✅ Filtered fixed positions:", fixed_pos_str)


print("✅ Fixed positions for ProteinMPNN:")
print(fixed_pos_str)


#@title Run ProteinMPNN to design new sequences for given backbone
import warnings, os, re
import pandas as pd
import numpy as np

from Bio.PDB import PDBParser, Superimposer, PDBIO
from Bio.PDB.StructureBuilder import StructureBuilder

model_name = "v_48_020"
pdb_path="/content/outputs/merged_open_monomer_and_closed_dimer.pdb"
chains = "A,B,C"
homooligomer = True
inverse = False
rm_aa = "C"
num_seqs = 512
sampling_temp = 0.1

print(f"Using chains: {chains}")
print(f"Fix positions (string): {fixed_pos_str}")
print(f"PDB path: {pdb_path}")
print("Checking chains in PDB...")

from inference.utils import parse_pdb
pdb_data = parse_pdb(pdb_path)
print("Available keys in parsed PDB:", pdb_data.keys())

# Extract chain IDs from pdb_idx
chain_ids = set(chain for chain, _ in pdb_data["pdb_idx"])
print(f"Chains found in PDB: {sorted(chain_ids)}")

# Optional: Compute number of residues per chain
from collections import Counter
chain_lengths = Counter(chain for chain, _ in pdb_data["pdb_idx"])
print(f"Chain lengths: {dict(chain_lengths)}")


# Get all valid (chain, res_id) from parsed PDB
existing_residues = set(pdb_data["pdb_idx"])

# Filter fix_pos to only include residues that are present
fixed_pos_str = ",".join([
    f"{chain}{res_id}" for (chain, res_id) in glucose_residues
    for chain in chains.split(",")
    if (chain, res_id) in existing_residues
])

print(f"Chain lengths: {chain_lengths}")


# Clean user options
chains = re.sub("[^A-Za-z]+",",", chains)
if fixed_pos_str == "": fixed_pos_str = None
rm_aa = ",".join(list(re.sub("[^A-Z]+","",rm_aa.upper())))
if rm_aa == "": rm_aa = None

# Load model and run ProteinMPNN
if "mpnn_model" not in globals():
  mpnn_model = mk_mpnn_model(model_name)

mpnn_model.prep_inputs(pdb_filename=pdb_path,
                       chain=chains, homooligomer=homooligomer,
                       fix_pos=fixed_pos_str, inverse=inverse,
                       rm_aa=rm_aa, verbose=True)

out = mpnn_model.sample(num=num_seqs//32, batch=32,
                        temperature=sampling_temp,
                        rescore=homooligomer)

# Save results
with open("design.fasta","w") as fasta:
  for n in range(num_seqs):
    line = f'>score:{out["score"][n]:.3f}_seqid:{out["seqid"][n]:.3f}\n{out["seq"][n]}'
    fasta.write(line+"\n")

data = [[out[k][n] for k in ["score","seqid","seq"]] for n in range(num_seqs)]
df = pd.DataFrame(data, columns=["score","seqid","seq"])
df.to_csv('output/mpnn_results.csv')


# Install dependencies and ColabDesign
# Install ColabDesign (sokrypton fork) and dependencies
!pip install --quiet git+https://github.com/sokrypton/ColabDesign.git
!pip install --quiet dm-haiku==0.0.14 jedi omegaconf hydra-core \
                         pyrsistent pynvml decorator
!pip install --quiet --no-dependencies dgl==2.0.0 -f https://data.dgl.ai/wheels/cu121/repo.html
!pip install --quiet --no-dependencies e3nn==0.5.5 opt_einsum_fx



import jax
jax.devices()



import os, sys
import matplotlib.pyplot as plt
from colabdesign.af import mk_af_model
from colabdesign.shared.protein import pdb_to_string
from colabdesign.shared.parse_args import parse_args

import pandas as pd
import numpy as np
from string import ascii_uppercase, ascii_lowercase
alphabet_list = list(ascii_uppercase + ascii_lowercase)

def get_info(contig):
  F = []
  free_chain = False
  fixed_chain = False
  sub_contigs = [x.split("-") for x in contig.split("/")]
  for n, (a, b) in enumerate(sub_contigs):
    if a[0].isalpha():
      L = int(b) - int(a[1:]) + 1
      F += [1] * L
      fixed_chain = True
    else:
      L = int(b)
      F += [0] * L
      free_chain = True
  return F, [fixed_chain, free_chain]

def run_validation(pdb, loc, contigs, copies, csv_path):
  argv = [
    f"--pdb={pdb}",
    f"--loc={loc}",
    f"--contigs={contigs}",
    f"--copies={copies}",
    "--num_seqs=512",
    "--num_recycles=3",
    "--rm_aa=C",
    f"--seq_csv={csv_path}",
    "--mpnn_sampling_temp=0.1",
    "--initial_guess",
    "--use_multimer"
  ]
  return main(argv)

def plot_ca_trace(pdb_file):
  from Bio.PDB import PDBParser
  parser = PDBParser(QUIET=True)
  structure = parser.get_structure("x", pdb_file)
  coords = [atom.coord for atom in structure.get_atoms() if atom.get_name() == "CA"]
  coords = np.array(coords)
  if len(coords) == 0:
    return ""
  fig = plt.figure(figsize=(1,1), dpi=50)
  ax = fig.add_subplot()
  ax.plot(coords[:,0], coords[:,1], lw=0.5)
  ax.axis("off")
  fig.canvas.draw()
  img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
  img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))
  plt.close(fig)
  return img

def main(argv):
  ag = parse_args()
  ag.txt("-------------------------------------------------------------------------------------")
  ag.txt("Designability Test")
  ag.txt("-------------------------------------------------------------------------------------")
  ag.txt("REQUIRED")
  ag.txt("-------------------------------------------------------------------------------------")
  ag.add(["pdb="], None, str, ["input pdb"])
  ag.add(["loc="], None, str, ["location to save results"])
  ag.add(["contigs="], None, str, ["contig definition"])
  ag.txt("-------------------------------------------------------------------------------------")
  ag.txt("OPTIONAL")
  ag.txt("-------------------------------------------------------------------------------------")
  ag.add(["copies="], 1, int, ["number of repeating copies"])
  ag.add(["num_seqs="], 512, int, ["number of sequences to evaluate"])
  ag.add(["initial_guess"], False, None, ["initialize previous coordinates"])
  ag.add(["use_multimer"], False, None, ["use alphafold_multimer_v3"])
  ag.add(["use_soluble"], False, None, ["use solubleMPNN"])
  ag.add(["num_recycles="], 3, int, ["number of recycles"])
  ag.add(["rm_aa="], "C", str, ["disable specific amino acids from being sampled"])
  ag.add(["num_designs="], 1, int, ["number of designs to evaluate"])
  ag.add(["seq_csv="], None, str, ["CSV file with sequences to use"])
  ag.add(["mpnn_sampling_temp="], 0.1, float, ["sampling temperature"])
  ag.txt("-------------------------------------------------------------------------------------")
  o = ag.parse(argv)

  if None in [o.pdb, o.loc, o.contigs]:
    ag.usage("Missing Required Arguments")

  if o.rm_aa == "":
    o.rm_aa = None

  contigs = []
  for contig_str in o.contigs.replace(" ", ":").replace(",", ":").split(":"):
    if len(contig_str) > 0:
      contig = [x for x in contig_str.split("/") if x != "0"]
      contigs.append("/".join(contig))

  chains = alphabet_list[:len(contigs)]
  info = [get_info(x) for x in contigs]
  fixed_pos = []
  for pos, _ in info:
    fixed_pos += pos

  flags = {
    "initial_guess": o.initial_guess,
    "best_metric": "rmsd",
    "use_multimer": o.use_multimer,
    "model_names": ["model_1_multimer_v3" if o.use_multimer else "model_1_ptm"]
  }

  protocol = "partial" if sum(fixed_pos) > 0 else "fixbb"
  print(f"protocol={protocol}")
  af_model = mk_af_model(protocol=protocol, use_templates=(protocol == "partial"), **flags)

  rm_template = np.array(fixed_pos) == 0
  prep_flags = {
    "chain": ",".join(chains),
    "copies": o.copies,
    "homooligomer": o.copies > 1,
    "rm_aa": o.rm_aa
  }
  if protocol == "partial":
    prep_flags.update({
      "rm_template": rm_template,
      "rm_template_seq": rm_template
    })

  if o.seq_csv is None:
    raise ValueError("--seq_csv is required to run validation without ProteinMPNN")
  df = pd.read_csv(o.seq_csv)
  seqs = df["seq"][:o.num_seqs].tolist()

  os.makedirs(f"{o.loc}/all_pdb", exist_ok=True)
  scores = []
  for n, seq in enumerate(seqs):
    af_model.prep_inputs(o.pdb, **prep_flags)
    af_model.predict(seq=seq, num_recycles=o.num_recycles, verbose=False)
    pdb_path = f"{o.loc}/all_pdb/design0_n{n}.pdb"
    af_model.save_current_pdb(pdb_path)
    af_model._save_results(save_best=False, verbose=False)
    af_model._k += 1
    log = af_model.aux["log"]
    scores.append({"seq": seq, "pdb": pdb_path, **{k: log.get(k, 0.0) for k in ["plddt", "ptm", "pae", "rmsd", "ipae"]}})

  return pd.DataFrame(scores)

if __name__ == "__main__":
  merged_pdb = "outputs/merged_open_monomer_and_closed_dimer.pdb"
  df_mono = run_validation(
    pdb=merged_pdb,
    loc="outputs/monomer",
    contigs="A1-461",
    copies=1,
    csv_path="outputs/monomer_designs.csv"
  )
  df_dimer = run_validation(
    pdb=merged_pdb,
    loc="outputs/dimer",
    contigs="B1-461:C1-461",
    copies=2,
    csv_path="outputs/dimer_designs.csv"
  )



!pip install --upgrade "jax[cuda12_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
!pip install cuda12_pip
import jax
print("JAX backend:", jax.lib.xla_bridge.get_backend().platform)




data = []
for i, (mon, dim) in enumerate(zip(df_mono.to_dict("records"), df_dimer.to_dict("records"))):
    custom_score = (np.mean([mon["rmsd"], dim["rmsd"]])
                    - 10*np.mean([mon["ptm"], dim["ptm"]])
                    - 10*np.mean([mon["plddt"], dim["plddt"]])
                    + np.mean([mon["pae"], dim["pae"]])/5
                    + dim.get("ipae", 0)/2)

    data.append({
      "seq": mon["seq"],
      "monomer_pdb": plot_ca_trace(mon["pdb"]),
      "dimer_pdb": plot_ca_trace(dim["pdb"]),
      "avg_rmsd": np.mean([mon["rmsd"], dim["rmsd"]]),
      "avg_ptm": np.mean([mon["ptm"], dim["ptm"]]),
      "avg_plddt": np.mean([mon["plddt"], dim["plddt"]]),
      "i_ptm": dim.get("ptm", 0),
      "i_pae": dim.get("ipae", 0),
      "plddt_diff": dim["plddt"] - mon["plddt"],
      "ptm_diff": dim["ptm"] - mon["ptm"],
      "rmsd_diff": dim["rmsd"] - mon["rmsd"],
      "custom_score": custom_score
    })

df_final = pd.DataFrame(data)
df_final.sort_values("custom_score", inplace=True)
df_final.to_csv("outputs/summary_scores.csv", index=False)
print("✅ Summary table written to outputs/summary_scores.csv")

!zip -r monomer.zip outputs/monomer*
!zip -r dimer.zip outputs/dimer* outputs/traj/{path}*
!zip -r result.zip monomer.zip dimer.zip
files.download(f"result.zip")

